<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>


  





<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="page-heading">
        
          <h1>Notes</h1>
          
            <p class="author">Mayukh Deb</p>

            
              <div class="abstract">
                <h5>Abstract</h5>
                <p>
                  This is a place to keep my notes, mostly from the papers I read and find interesting.
                </p>
              </div>
            
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
<div class="container" role="main">
    <div class="posts-list">
      
        
      

      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-19-07-locating-and-editing-factual-associations-in-gpt/">
      <h2 class="post-title">Locating and Editing Factual Knowledge in GPTs</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Jul 19, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>This paper&rsquo;s approach has been to develop a mechanism to identify the neuron activations that lead to a model&rsquo;s factual predictions and possibly even edit existing facts.
Key concepts and takeaways: What is a fact tuple? It is a special way to store knowledge in the form of a tuple which looks like the following:
## (subject, relation, object) (&#34;Edmunt Neupert&#34;, &#34;Plays the instrument&#34;, &#34;Piano&#34;) It contains the subject (index 0), the object (index 2) and their relation (index 1).</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-19-07-locating-and-editing-factual-associations-in-gpt/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-14-07-lms-mostly-know-what-they-know/">
      <h2 class="post-title">Language Models (mostly) Know What they know</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Jul 14, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>Can we make LMs predict which questions they&rsquo;ll be able to answer correctly? It is important for LLMs to &ldquo;know&rdquo; what they know and what they do not know. The problem is that LMs are generally never trained to say &ldquo;I do not know&rdquo;. But it might be possible to quantify this ability post-training.
This is how they approach the problem:
 Finetune models with a value head to predict the probabiility that they can answer a given question correctly.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-14-07-lms-mostly-know-what-they-know/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-04-16-attention_flow/">
      <h2 class="post-title">Attention Rollout</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Apr 16, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>How is it better than just viewing raw attention maps ? Viewing raw attention maps as a way to explain transformers does not take into account the fact that we also have residual connections in the model. When we only use attention weights to approximate the flow of information in Transformers, we ignore the residual connections. But these connections play a significant role in tying corresponding positions in different layers.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-04-16-attention_flow/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
      </div>
    
    <ul class="pager">
      
        <li class="previous">
          <a href="https://mayukhdeb.github.io/notes/page/1/">&larr; Newer</a>
        </li>
      
      
        <li class="next">
          <a href="https://mayukhdeb.github.io/notes/page/3/">Older &rarr;</a>
        </li>
      
    </ul>
  
</div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about">Mayukh Deb</a>
        &copy;
        2022
      </p>
  </div>
</footer></body>
</html>
