<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>past on Writing</title>
    <link>https://mayukhdeb.github.io/writing/categories/past/</link>
    <description>Recent content in past on Writing</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 24 Oct 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mayukhdeb.github.io/writing/categories/past/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Hello World</title>
      <link>https://mayukhdeb.github.io/writing/post/2022-04-16-attention_flow/</link>
      <pubDate>Sun, 24 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/writing/post/2022-04-16-attention_flow/</guid>
      <description>Attention Rollout for explaining transformers How is it better than just viewing raw attention maps ? Viewing raw attention maps as a way to explain transformers does not take into account the fact that we also have residual connections in the model. When we only use attention weights to approximate the flow of information in Transformers, we ignore the residual connections. But these connections play a significant role in tying corresponding positions in different layers.</description>
    </item>
    
  </channel>
</rss>