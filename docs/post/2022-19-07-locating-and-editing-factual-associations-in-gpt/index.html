<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>Locating and Editing Factual Knowledge in GPTs - Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>






<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Locating and Editing Factual Knowledge in GPTs</h1>
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
  <div class="container" role="main">
    <article class="article" class="blog-post">
      <div class="postmeta">
        <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Jul 19, 2022
  
</span>
      </div>
      <br>
      
    <p>This paper&rsquo;s approach has been to develop a mechanism to identify the neuron activations that lead to a model&rsquo;s factual predictions and possibly even edit existing facts.</p>
<h2 id="key-concepts-and-takeaways">Key concepts and takeaways:</h2>
<h3 id="what-is-a-fact-tuple"><strong>What is a fact tuple?</strong></h3>
<p>It is a special way to store knowledge in the form of  a tuple which looks like the following:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">## (subject, relation, object)</span>
(<span style="color:#e6db74">&#34;Edmunt Neupert&#34;</span>, <span style="color:#e6db74">&#34;Plays the instrument&#34;</span>, <span style="color:#e6db74">&#34;Piano&#34;</span>)
</code></pre></div><p>It contains the subject (index 0), the object (index 2) and their relation (index 1).</p>
<h3 id="knowing-differs-from-saying"><strong>Knowing differs from saying</strong></h3>
<p>Just because an LLM &ldquo;knows&rdquo; a fact, does not always mean that it&rsquo;ll state it in the completion. This is because transformers are pretty sensitive to the prompt itself.</p>
<p>Just how LLMs can <strong>say</strong> something without actually <strong>knowing</strong> it, it can also <strong>know</strong> something without <strong>saying</strong> it.</p>
<h3 id="what-is-knowledge-in-a-llm-if-it-exists-is-there-an-elementary-unit-of-knowledge-within-the-models"><strong>What is knowledge in a LLM? If it exists, is there an elementary unit of knowledge within the models?</strong></h3>
<p><strong>What if there was some specific computation within the LLM which plays a devcisive role for a given fact?</strong></p>
<p>It is fair to assume that a certain fact stored within the model is  evenly distributed among all of the computations being performed during the forward pass, but that is generally not the case. It turns out that when we corrupt the embeddings of certain input tokens, the probability of the original output changes drastically.</p>
<p>The question is, which part of the model and the prompt was responsible for this decisive change?</p>
<p>Let&rsquo;s try to understand this with an example:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">prompt <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Miles Davis plays the&#34;</span>
</code></pre></div><p>Now on a normal GPT2-XL, the completion would be <code>piano</code> because Miles Davis was indeed a famous piano player.</p>
<p>The catch is, now when we &ldquo;corrupt&rdquo; the embeddings of the subject i.e <code>Miles</code> and <code>Davis</code>, then the output gets corrupted as well (i.e the probability of <code>piano</code> decreases)</p>
<p><strong>Side note: What do we mean by corrupting an input subject?</strong></p>
<p>The authors corrput the subject in the prompt by adding noise to the embeddings for all tokens in the prompt that refer to the subject entity (which in our case are 2 tokens <code>[&quot;Miles&quot;, &quot;Davis&quot;]</code>)</p>
<p>Since the input is now corrupted for some tokens along the seq dim, the hidden states of the model also get altered. In order to find which part(s) of the model was responsible for the original output (<code>piano</code>), we can now replace the specific &ldquo;corrupted hidden states&rdquo; with the original &ldquo;clean hidden states&rdquo; and see which combination leads to the highest probability of the original output.</p>
<p>(WIP)</p>
<h2 id="resources">Resources</h2>
<ol>
<li>The paper (of course): <a href="https://arxiv.org/pdf/2202.05262.pdf">https://arxiv.org/pdf/2202.05262.pdf</a></li>
<li>Website: <a href="https://rome.baulab.info/">https://rome.baulab.info/</a></li>
<li>Nice and elaborate walkthrough by the folks at Eleuther: <a href="https://www.youtube.com/watch?v=IkbYu_poZVE">https://www.youtube.com/watch?v=IkbYu_poZVE</a></li>
</ol>



      
        <div class="blog-tags">
          
            <a href="https://mayukhdeb.github.io/notes//tags/paper/">paper</a>&nbsp;
          
        </div>
      
    </article>
    
  </div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about">Mayukh Deb</a>
        &copy;
        2022
      </p>
  </div>
</footer></body>
</html>
