<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>The Third Wave (?) - Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>






<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>The Third Wave (?)</h1>
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
  <div class="container" role="main">
    <article class="article" class="blog-post">
      <div class="postmeta">
        <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Aug 1, 2022
  
</span>
      </div>
      <br>
      
    <p>I will start by rambling a little about how <a href="https://garymarcus.substack.com/p/learning-language-is-harder-than">learning language is harder than you think</a>. Then I&rsquo;ll move on to the main paper itself.</p>
<h2 id="learning-language-is-harder-than-you-think">Learning language is harder than you think</h2>
<p>An interesting point that was made in the article was that a language is learned <em>not</em> entirely through memorization. Sentences like <em>I breaked the window</em> are grammatically invalid, but they still manage to get the meaning accross. We manage to make a meaningful fact tuple i.e <code>({person}, break, window)</code>. And the sentence <em>I breaked the window</em> is just a crude way to imply that exactly that.</p>
<p>LLMs like that of GPT-3 are really good at generating text which looks &ldquo;realistic&rdquo;. But they face the problem that DALLE-2 faces with compositionality.</p>
<p>The reasons as to why DALLE-2 cannot consistently make an image of a red box on a blue box might just be the same as to why GPT-3 cannot stuff like logical reasoning (sure it can solve small riddles, but what about longer chains of thought?).</p>
<p>Another important point that was made was that children require far less amount of &ldquo;training data&rdquo; when compared to that of LLMs to pick up their language.</p>
<p><em>&ldquo;To date, nobody, ever, has given a convincing and thorough account of how human children (and human children alone) learn language. To get there, You would probably want a rich theory about how people represent meanings&rdquo;</em></p>
<p>The way we train LLMs today basically follows this principle: <em>&ldquo;Let&rsquo;s throw in as many words as possible into the neural net and make it predict the next one in hopes that this will somehow make it smart.&quot;</em></p>
<p>My personal opinion is that these LLMs are just really really good clever hanses. They are very good at being grammatically fluent, but that does not imply that they&rsquo;re a good model for the human mind. They are like very good parrots, knowing how to arrange words in the right order, but never to actually understand them.</p>
<p>GPTs do not tell us things about the world, it simply imitates the patterns of human language. It has no form of reasoning built into it.</p>
<h2 id="the-third-wave">The Third wave</h2>
<p>The solution to some of the drawbacks of current deep-learning method might just emerge from something called &ldquo;Neurosymbolic AI&rdquo;.</p>
<p><strong>What is neurosymbolic AI?</strong></p>
<p>It is an alternative approach to build systems which not just classify/predict certain variables, but also have some sort of an internal mechanism of logical reasoning. It&rsquo;s a way to make neural networks &ldquo;think step by step&rdquo; so that they generalise with lower amounts of data.</p>
<p><strong>AI cookbook</strong></p>
<p>Gary Marcus proposes that we should build a system that can acquire, represent, and manipulate abstract knowledge.</p>
<p>In order to be actually smart, AGI would require a form of symbolic manipulation of variables in logic.</p>
<p>(WIP)
(WIP)</p>



      
        <div class="blog-tags">
          
            <a href="https://mayukhdeb.github.io/notes//tags/paper/">paper</a>&nbsp;
          
        </div>
      
    </article>
    
  </div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about">Mayukh Deb</a>
        &copy;
        2022
      </p>
  </div>
</footer></body>
</html>
