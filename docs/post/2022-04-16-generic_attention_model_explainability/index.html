<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>Generic Attention-model Explainability - Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>






<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Generic Attention-model Explainability</h1>
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
  <div class="container" role="main">
    <article class="article" class="blog-post">
      <div class="postmeta">
        <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Apr 16, 2022
  
</span>
      </div>
      <br>
      
    <h2 id="generic-attention-model-explainability-for-interpreting-bi-modal-and-encoder-decoder-transformers">Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers</h2>
<p>Can be used to explain models like CLIP. This is how it works:</p>
<ol>
<li>
<p>First, let us define an input image <code>x_image</code> and a list of input texts <code>[a, b, c]</code> where <code>a</code>, <code>b</code> and <code>c</code> can be any strings which can be tokenized and fed into the model.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">input_texts <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;a bald man&#34;</span>, <span style="color:#e6db74">&#34;a rocket in space&#34;</span>, <span style="color:#e6db74">&#34;a man&#34;</span>]
</code></pre></div></li>
<li>
<p>We do a forward pass with a tokenized image and text(s) on CLIP, and obtain <code>logits_per_image</code> and <code>logits_per_text</code>. Each item within these tensors correspond to <code>a</code> and <code>b</code> and <code>c</code> respectvely.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">{
    <span style="color:#e6db74">&#39;logits_per_image&#39;</span>: tensor([[<span style="color:#ae81ff">22.7188</span>, <span style="color:#ae81ff">16.1094</span>, <span style="color:#ae81ff">23.4219</span>]]), <span style="color:#75715e">## shape: 1,3</span>
    <span style="color:#e6db74">&#39;logits_per_text&#39;</span>: tensor([[<span style="color:#ae81ff">22.7344</span>],[<span style="color:#ae81ff">16.1250</span>],[<span style="color:#ae81ff">23.4219</span>]]) <span style="color:#75715e">## shape: 3,1</span>
}
</code></pre></div></li>
<li>
<p>We run a softmax over <code>logits_per_image</code> on the last dim and get a nice probability map with a sum of 1 (we also convert it to numpy). Let&rsquo;s call it <code>probs</code>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">{
    <span style="color:#e6db74">&#39;probs&#39;</span>: array([[<span style="color:#ae81ff">3.311e-01</span>, <span style="color:#ae81ff">4.461e-04</span>, <span style="color:#ae81ff">6.685e-01</span>]]
}
</code></pre></div></li>
<li>
<p>We then choose a target <code>index</code> w.r.t which we want to find the relevance mapping. The <code>index</code> here refers to the index of one the input texts which refers to <code>a</code>, <code>b</code> or <code>c</code>.  For now, let&rsquo;s set <code>index = 0</code></p>
</li>
<li>
<p>Now, we create a tensor with all zeros called <code>one_hot</code> which has the same shape as <code>logits_per_image</code> i.e <code>(1, 3)</code> and set <code>one_hot[0, index] = 0</code>. This tensor has <code>requires_grad</code> set to true which will come to play later on:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">{
    <span style="color:#e6db74">&#39;one_hot&#39;</span>: tensor([[<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">0.</span>]], requires_grad<span style="color:#f92672">=</span>True)
}
</code></pre></div></li>
<li>
<p>We calculate a &ldquo;loss&rdquo; which is defined as shown below and do a backward pass over through the model. <code>one_hot * logits_per_image</code> masks <code>logits_per_image</code> to only include the logit corresponding to <code>index</code>. So in this case <code>loss = 22.7188</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sum(one_hot <span style="color:#f92672">*</span> logits_per_image)
model<span style="color:#f92672">.</span>zero_grad()
loss<span style="color:#f92672">.</span>backward(retain_graph<span style="color:#f92672">=</span>True)
</code></pre></div></li>
<li>
<p>Now, let us assume that we have <code>n</code> layers in the model with attention outputs. Out of which we select the last layer for our work, let us call it <code>block</code>. The output of the attention layer would be a 3D tensor of shape which looks something like: <code>[12, 50, 50]</code>.  We select a subset of these layers for our experiments.</p>
</li>
<li>
<p>We make an identity matrix <code>R</code> (probably means to &ldquo;results&rdquo;) of shape <code>(d , d)</code> where <code>d</code> is the length of the last dim of the attention outputs of the model. In this case, <code>d</code> is 50.</p>
</li>
<li>
<p>Using forward and backward hooks, we capture the outputs of the attention layer(s) into 2 attributes both of which have shape <code>(12, 50, 50)</code>:</p>
<ul>
<li><code>block.attn_probs</code> (forward hook): stores the attention outputs post softmax + dropout.</li>
<li><code>block.attn_grad</code> (backward hook): stores the gradients of the same outputs w.r.t the loss. It tells us <code>d(loss)/d(block.attn_probs)</code>.</li>
</ul>
</li>
<li>
<p>we do the following with <code>block.attn_probs</code> and <code>block.attn_grad</code> iteratively for each <code>block</code>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">y <span style="color:#f92672">=</span> block<span style="color:#f92672">.</span>attn_probs <span style="color:#f92672">*</span> block<span style="color:#f92672">.</span>attn_grad  <span style="color:#75715e">## y.shape = (12, 50, 50)</span>
y <span style="color:#f92672">=</span> cam<span style="color:#f92672">.</span>clamp(min<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  <span style="color:#75715e">## y.shape = (50,50)</span>
R <span style="color:#f92672">+=</span> torch<span style="color:#f92672">.</span>matmul(cam, R)  <span style="color:#75715e">## R.shape = (50,50) </span>
</code></pre></div></li>
<li>
<p>We then set <code>R[0, 0] = 0</code> for some reason which I&rsquo;m not sure of rn.</p>
</li>
<li>
<p>Then we obtain a 2D heatmap by selecting a specific subset of <code>R</code> which is <code>R[0, 1:]</code> (which comes to be of shape <code>(49,)</code>) and reshape it to <code>(7,7)</code>.</p>
</li>
<li>
<p>Finally, we scale this heatmap and resize it to be of the same height and width as of the original input image.</p>
</li>
</ol>
<h2 id="resources">Resources:</h2>
<ul>
<li><a href="https://arxiv.org/pdf/2103.15679.pdf">original paper</a></li>
<li><a href="https://github.com/hila-chefer/Transformer-MM-Explainability">original implementation on github</a></li>
</ul>



      
        <div class="blog-tags">
          
            <a href="https://mayukhdeb.github.io/notes//tags/paper/">paper</a>&nbsp;
          
        </div>
      
    </article>
    
  </div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about">Mayukh Deb</a>
        &copy;
        2022
      </p>
  </div>
</footer></body>
</html>
